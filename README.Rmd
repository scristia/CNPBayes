---
title: "CNPBayes: Bayesian copy number detection and association in large-scale studies"
author: "Stephen Cristiano, Jacob Carey, David McKean, Gary L. Rosner, Ingo Ruczinski, Alison Klein, and Robert B. Scharpf"
date: "`r format(Sys.Date())`"
output:	
  html_document:
    keep_md: true
bibliography: refs.bib	
---

# Overview

Germline copy number variants (CNVs) increase risk for many diseases, yet detection of CNVs and quantifying their contribution to disease risk in large-scale studies is challenging due to biological and technical sources of heterogeneity that vary across the genome within and between samples. We developed an approach called CNPBayes to identify latent batch effects in genome-wide association studies involving copy number, to provide probabilistic estimates of integer copy number across the estimated batches, and to fully integrate the copy number uncertainty in the association model for disease. 

# Installation

```{r installation, eval=FALSE}
## install.packages("devtools")
devtools::install_github("scristia/CNPBayes")
## gap has various functions, some Bayesian, for computing Hardy Weinberg equilibrium
install.packages("gap")
```

# Usage and workflow

## Example data

The simplest way to illustrate how to fit finite normal mixture models in CNPBayes is to start with a `SummarizedExperiment` object provided in CNPBayes that contains median summaries of a few CNV regions for individuals analyzed as part of the Pancreatic Cancer Case Control Consortium (PanC4) @Childs2015.  Here, we will focus on a CNV region on chromosome 8:

```{r multibatch_data}
library(tidyverse)
library(CNPBayes)
library(SummarizedExperiment)
library(ggplot2)
library(rjags)
library(ggmcmc)
extdir <- system.file("extdata", package="CNPBayes")
cnp_se <- readRDS(file.path(extdir, "cnp_se.rds"))
snp_se <- readRDS(file.path(extdir, "snp_se.rds"))
rowRanges(cnp_se)[5]
se <- cnp_se[5, ]
```

## Identifying possible sources of batch effects

As in any large-scale study, sources of technical variation can potentially confound statistical inference @Leek2010. CNPBayes estimates the batches for a given CNV region starting with a provisional definition of batch (chemistry plate on which the samples were processed, the date genomic libraries were prepared, etc.) provided by the user.  Leaving these details  the [vignette](vignettes/cnpbayes.Rmd), here we focus on the logistics of fitting a mixture model in CNPBayes and the subsequent genotyping. The following step uses the `median_summary` function extracts the median log$_2 R$ ratio from the `SummarizedExperiment` object and down-samples the observed data since we can derive useful approximations of the mixture components without all 6000 individuals.  Since the down-sampling is random, it is important to set a seed for reproducibility.

```{r median_summary, message=FALSE}
set.seed(1234)
cnv.summaries <- median_summary(se,
                                provisional_batch=se$Sample.Plate,
                                THR=-1) %>%
    down_sample(S=1000) 
cnv.summaries
## cnv.summaries <- kolmogorov_batches(cnv.summaries, KS_cutoff=0.001, THR=-1)
```

Below, we visualize the down-sampled data and circle the observations in the left-tail that we have flagged as possibly homozygous deletions.

```{r mixtures, fig.align="center", fig.width=8, fig.height=6}
##hd <- tibble(oned=min(oned(mb)))
hd <- filter(cnv.summaries, likely_deletion)
cnv.summaries %>%
    ggplot(aes(oned, ..count..)) +
    geom_histogram(aes(oned, ..density..), bins=200) +
    geom_point(data=hd,
               aes(oned, 0),
               shape=21,
               fill="transparent",
               size=7,
               color="steelblue")  +
    theme_bw() +
    xlab("Median log2 R ratio")
```

## Fitting finite mixture models

The bulk of the data corresponds to diploid individuals with median log$_2 R$ ratios near zero, and the small cluster to the left of this model contains subjects with a hemizygous deletion.  At the very left end of the plot are 6 individual with a likely homozygous deletion that we've circled. Below, we first create an object of class `MultiBatch` that organizes the data in a container that contains parameters for the mixture models.  While the code below instantiates a `MultiBatch` object with 3 components, the only critical piece at this step is the creation of a `MultiBatch` object.  While model selection can be challenging and requires evaluating many models, here we sidestep these issues and tell CNPBayes to focus on models that would be consistent with the deletion polymorphism evident from the preceding figure. Additional details regarding model selection are provided in the vignette.

```{r deletion_models, message=FALSE, cache=TRUE}
## Assume batch effects effects are neglible and that there is a single batch
mb <- MultiBatch("MB3", data=cnv.summaries)
mp <- McmcParams(burnin=100, iter=1000, thin=1)
model <- homdel_model(mb, mp, THR=-1)
model
```

The resulting `model` object contains information about the type of model that was fit and the number of mixture components.  Here `SB3` means that the selected model has a single batch (SB) with 3 mixture components.  If the extreme observations in the left-tail were more rare (or we were fitting a multi-batch model), CNPBayes has an experimental feature where we augment the observed data in the left tail to reinforce our belief that a mixture component should be dedicated to these observations as opposed to a heavy-tailed distribution.  Again, we leave the details to the vignette. To evaluate goodness of fit, we overlay the density of the posterior predictive distribution on the empirical data using the `ggMixture` function, again circling the observed homozygous deletions.

```{r posterior_predictive, fig.align="center", fig.cap="Posterior predictive distribution from CNPBayes overlaying the median log2 R ratios.", fig.width=8, fig.height=6}
ggMixture(model) +
    xlab("median log R ratio")
```

Tools for assessing convergence are provided in the vignette.

## Genotyping the mixture components

While we have assigned each sample to a mixture component, the mixture components do not necessarily correspond to distinct copy number states.  Using the available SNPs in the CNV region, we identify the set of integer copy numbers that would most likely give rise to the observed B allele frequencies (BAFs). After limiting the SNP `SummarizedExperiment` object to the CNV region of interest, we call the `genotype_model` function to map mixture components to integer copy number.  Using the `mapping` accessor, we see that the three mixture components were mapped to the integer copy numbers 0, 1, and 2.

```{r genotype}
snp.chr8 <- subsetByOverlaps(snp_se, se)
gmodel <- genotype_model(model, snp.chr8)
mapping(gmodel)
```

As we fit the mixture model using a subset of the available data, we extrapolate the probabilistic estimates of copy number to the entire population using the `upsample` function.  While we did not make use of the provisional batch labels in this simple example, the up-sampling does require that we provide these labels from the full data.


```{r upsampling, cache=TRUE}
full <- upsample(gmodel, se,
                 provisional_batch=se$Sample.Plate)
full
freq <- as.integer(table(full$copynumber))
freq
pval <- gap::hwe(freq, data.type="count")$p.x2 
```

From the above analyses, we find that the frequencies of copy number states 0, 1, and 2 in this dataset (`r paste(freq, collapse=", ")`) are consistent with a deletion allele segregating at Hardy Weinberg equilibrium in the population.

# Association model

If the mixture components were always as well separated as above, standard Bayesian and frequentist regression models using the maximum a posteriori copy number estimate would be appropriate.  To illustrate the approach using a toy example, we simulate disease status for 1000 observations.


```{r simulate_disease}
b0 <- 1.5
b1 <- -0.75
map_cn_estimates <- full$copynumber[1:1000]
XB <- b0 + b1 * map_cn_estimates
probs <- exp(XB)/(1 + exp(XB))
y  <- rbinom(length(probs), 1, prob=probs)
df <- tibble(y=y, cn=map_cn_estimates)
fit1 <- glm(y~cn, data=df, family=binomial(link="logit"))
coef(summary(fit1))
glmbeta <- coef(summary(fit1))[2, "Estimate"]
```

When copy number estimates are uncertain, a Bayesian logistic regression model can easily incorporate the uncertainty of the latent copy number in the regression coefficient standard errors. We include a simple model in JAGS (without other covariates), passing the posterior probabilities of the integer copy number assignments to the JAGS model in the variable `P`.

```{r uncertainty_of_cn, results="hide", message=FALSE, cache=TRUE}
cn_probs <- ungroup(full[1:1000, ]) %>%
    select(c("cn_0", "cn_1", "cn_2")) %>%
    as.matrix()
jags_data <- list(N=length(y),
                  y=y,
                  P=cn_probs)
jagsdir <- system.file("JAGS", package="CNPBayes")
fit <- jags.model(file.path(jagsdir, "cnv_assoc.jag"),
                  data=jags_data,
                  n.chains=1,
                  n.adapt=500)
samples <- coda.samples(fit,
                        variable.names=c("b0", "b1", "zbeta"),
                        n.iter=2000*50, thin=50) %>%
    ggs()
```

Our jags model includes a latent indicator variable $z$ that multiplies the copy number coefficient. This is essentially Bayesian model averaging where the two models of interest are an intercept-only model and a model that includes copy number.  Below, we show the traceplot of the regression coefficient when $z$ is 1:

```{r posterior_summaries, fig.align="center", fig.cap="We used a Bayesian model averaging approach to choose between models where the regression coefficient for copy number was non-zero and an intercept-only model.  Here, we show the traceplot for beta when the copy number model was selected.  The horizontal line is the estimate of the regression coefficient from the `glm` model.", fig.width=8, fig.height=3}
b <- filter(samples, Parameter=="zbeta")
b %>%
    filter(value != 0) %>%
    ggs_traceplot() +
    geom_line(color="gray")+
    geom_hline(yintercept=glmbeta, color="steelblue") +
    theme_bw()
```

Due to the autocorrelation in this chain, regions of the traceplot near where $z$ was zero are still evident -- additional thinning and more iterations would be required to provide a useful posterior for the $\beta_{cn}$ coefficient. An advantage of this approach is that we get a direct estimate of the probability that the regression coefficient for copy number is non-zero (probability `r mean(b$value!=0)`).


# Related software

cnvCall fits Bayesian hierarchical models of t-distributions assuming the principal sources of batch effects are known (@Cardin2011), building on and extending many of the ideas for modeling copy number variation in the R package `CnvTools` (@Barnes2008).  Expection-Maximization implementations of mixture models are available in the canary package of the Birdsuite software (@Korn2008).  Mixture model based approaches for modeling copy number at specific regions of the genome have also been useful in whole exome sequencing application (e.g., @Fromer2012 and others).

# References

